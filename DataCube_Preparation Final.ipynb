{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74ggczkfAMqi"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    !pip install --upgrade xee"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rioxarray"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OEn3k0Ti_gz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tAojHY4iAU8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path1 = '/content/drive/MyDrive/Amini'"
      ],
      "metadata": {
        "id": "5Ouzjs3EAnIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# search for .tif files\n",
        "tif_files = glob.glob(os.path.join(path1, '*.tif'))\n",
        "\n",
        "# Print the list of .tif files\n",
        "if tif_files:\n",
        "    print(\"Found .tif files:\")\n",
        "    for file in tif_files:\n",
        "        print(file)\n",
        "else:\n",
        "    print(\"No .tif files found in the directory.\")\n"
      ],
      "metadata": {
        "id": "_rw3SPUoA2sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import rioxarray as rxr\n",
        "import xarray as xr\n",
        "import numpy as np\n",
        "\n",
        "# Define your data directory\n",
        "data_dir = \"/content/drive/MyDrive/Amini\"\n",
        "\n",
        "# Find all .tif files in the directory\n",
        "tif_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\".tif\")]\n",
        "\n",
        "# Regular expressions to match time periods and variable names\n",
        "time_pattern = re.compile(r\"(Dec_2024|Jan_2025|Feb_2025)\")\n",
        "var_pattern = re.compile(r\"(Sentinel1_VV|Sentinel1_VH|NDVI|LST)\")\n",
        "\n",
        "# Dictionary to store datasets\n",
        "datasets = {}\n",
        "\n",
        "print(\"\\n Found .tif files:\")\n",
        "for file in tif_files:\n",
        "    filename = os.path.basename(file)\n",
        "    time_match = time_pattern.search(filename)\n",
        "    var_match = var_pattern.search(filename)\n",
        "\n",
        "    if time_match and var_match:\n",
        "        time_label = time_match.group(1)\n",
        "        var_label = var_match.group(1)\n",
        "\n",
        "        # Create a nested dictionary structure\n",
        "        if time_label not in datasets:\n",
        "            datasets[time_label] = {}\n",
        "\n",
        "        # Load the raster dataset\n",
        "        try:\n",
        "            ds = rxr.open_rasterio(file).squeeze()\n",
        "            ds = ds.rename(\"value\")  # Ensure naming consistency\n",
        "            datasets[time_label][var_label] = ds\n",
        "            print(f\" Matched: {filename} → Time: {time_label}, Variable: {var_label}\")\n",
        "        except Exception as e:\n",
        "            print(f\" Error loading {filename}: {e}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"⚠ Skipped: {filename} (No match on time/variable regex)\")\n",
        "\n",
        "# Check if we have successfully organized datasets\n",
        "print(\"\\n Organized Datasets:\")\n",
        "for time_label, variables in datasets.items():\n",
        "    print(f\" {time_label}: {list(variables.keys())}\")\n",
        "\n",
        "#  Check dataset dimensions before stacking\n",
        "print(\"\\n Dataset Dimensions Before Stacking:\")\n",
        "for time_label, variables in datasets.items():\n",
        "    for var_label, ds in variables.items():\n",
        "        print(f\" {time_label} - {var_label}: {ds.shape}\")\n",
        "\n",
        "# Ensure all variables exist for each time period (avoid missing data issues)\n",
        "all_variables = {\"Sentinel1_VV\", \"Sentinel1_VH\", \"NDVI\", \"LST\"}\n",
        "for time_label in datasets.keys():\n",
        "    missing_vars = all_variables - set(datasets[time_label].keys())\n",
        "    for var in missing_vars:\n",
        "        print(f\"⚠ Warning: {time_label} is missing {var}. Filling with NaNs.\")\n",
        "        example_shape = next(iter(datasets[time_label].values())).shape\n",
        "        datasets[time_label][var] = xr.DataArray(np.full(example_shape, np.nan), dims=(\"y\", \"x\"))\n",
        "\n",
        "#  Stack datasets into a single xarray DataArray\n",
        "stacked_datasets = []\n",
        "time_labels = sorted(datasets.keys())  # Sort to maintain proper time order\n",
        "\n",
        "for time_label in time_labels:\n",
        "    try:\n",
        "        time_ds = xr.concat([datasets[time_label][var] for var in sorted(all_variables)], dim=\"variable\")\n",
        "        time_ds = time_ds.assign_coords(time=time_label, variable=list(sorted(all_variables)))\n",
        "        stacked_datasets.append(time_ds)\n",
        "    except ValueError as e:\n",
        "        print(f\" Error concatenating datasets for {time_label}: {e}\")\n",
        "\n",
        "# Final merging of all time periods\n",
        "try:\n",
        "    final_datacube = xr.concat(stacked_datasets, dim=\"time\")\n",
        "    final_datacube = final_datacube.assign_coords(time=time_labels)  # Ensure proper time labels\n",
        "    print(\"\\n Successfully stacked all datasets into a DataArray!\")\n",
        "except ValueError as e:\n",
        "    print(f\" Final concatenation error: {e}\")\n",
        "    final_datacube = None\n",
        "\n",
        "# Save the final dataset as NetCDF\n",
        "if final_datacube is not None and not final_datacube.isnull().all():\n",
        "    output_path = os.path.join(data_dir, \"datacube.nc\")\n",
        "    final_datacube.to_netcdf(output_path)\n",
        "    print(f\"\\n Saved final dataset to: {output_path}\")\n",
        "else:\n",
        "    print(\"\\n No final dataset created due to errors.\")\n"
      ],
      "metadata": {
        "id": "vR0l2eD7BxE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import re\n",
        "# import rioxarray as rxr\n",
        "# import xarray as xr\n",
        "# import numpy as np\n",
        "\n",
        "# # Define your data directory\n",
        "# data_dir = \"/content/drive/MyDrive/Amini Africa\"\n",
        "\n",
        "# # Find all .tif files in the directory\n",
        "# tif_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\".tif\")]\n",
        "\n",
        "# # Regular expressions to match time periods and variable names\n",
        "# time_pattern = re.compile(r\"(Dec_2024|Jan_2025|Feb_2025)\")\n",
        "# var_pattern = re.compile(r\"(Sentinel1_VV|Sentinel1_VH|NDVI|LST|SRTM_DEM)\")\n",
        "\n",
        "# # Dictionary to store datasets\n",
        "# datasets = {}\n",
        "\n",
        "# print(\"\\nFound .tif files:\")\n",
        "# for file in tif_files:\n",
        "#     filename = os.path.basename(file)\n",
        "#     time_match = time_pattern.search(filename)\n",
        "#     var_match = var_pattern.search(filename)\n",
        "\n",
        "#     if time_match and var_match:\n",
        "#         time_label = time_match.group(1)\n",
        "#         var_label = var_match.group(1)\n",
        "\n",
        "#         # Create a nested dictionary structure\n",
        "#         if time_label not in datasets:\n",
        "#             datasets[time_label] = {}\n",
        "\n",
        "#         # Load the raster dataset\n",
        "#         try:\n",
        "#             ds = rxr.open_rasterio(file).squeeze()\n",
        "#             ds = ds.rename(\"value\")  # Ensure naming consistency\n",
        "#             datasets[time_label][var_label] = ds\n",
        "#             print(f\"Matched: {filename} → Time: {time_label}, Variable: {var_label}\")\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error loading {filename}: {e}\")\n",
        "\n",
        "#     else:\n",
        "#         print(f\"⚠ Skipped: {filename} (No match on time/variable regex)\")\n",
        "\n",
        "# # Check if we have successfully organized datasets\n",
        "# print(\"\\nOrganized Datasets:\")\n",
        "# for time_label, variables in datasets.items():\n",
        "#     print(f\"{time_label}: {list(variables.keys())}\")\n",
        "\n",
        "# # Check dataset dimensions before stacking\n",
        "# print(\"\\nDataset Dimensions Before Stacking:\")\n",
        "# for time_label, variables in datasets.items():\n",
        "#     for var_label, ds in variables.items():\n",
        "#         print(f\"{time_label} - {var_label}: {ds.shape}\")\n",
        "\n",
        "# # Ensure all variables exist for each time period (avoid missing data issues)\n",
        "# all_variables = {\"Sentinel1_VV\", \"Sentinel1_VH\", \"NDVI\", \"LST\", \"SRTM_DEM\"}\n",
        "# for time_label in datasets.keys():\n",
        "#     missing_vars = all_variables - set(datasets[time_label].keys())\n",
        "#     for var in missing_vars:\n",
        "#         print(f\"⚠ Warning: {time_label} is missing {var}. Filling with NaNs.\")\n",
        "#         example_shape = next(iter(datasets[time_label].values())).shape\n",
        "#         datasets[time_label][var] = xr.DataArray(np.full(example_shape, np.nan), dims=(\"y\", \"x\"))\n",
        "\n",
        "# # Load the SRTM DEM file separately (since it's static across time)\n",
        "# srtm_files = [file for file in tif_files if \"SRTM_DEM\" in file]\n",
        "\n",
        "# if srtm_files:\n",
        "#     dem_path = srtm_files[0]  # Take the first found DEM file\n",
        "#     print(f\"\\nFound SRTM DEM: {dem_path}\")\n",
        "\n",
        "#     try:\n",
        "#         dem_ds = rxr.open_rasterio(dem_path).squeeze()\n",
        "#         dem_ds = dem_ds.rename(\"SRTM_DEM\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error loading SRTM DEM: {e}\")\n",
        "#         dem_ds = None\n",
        "# else:\n",
        "#     print(\"\\nNo SRTM DEM file found!\")\n",
        "#     dem_ds = None\n",
        "\n",
        "# # Ensure SRTM DEM is included for each time period\n",
        "# if dem_ds is not None:\n",
        "#     for time_label in datasets.keys():\n",
        "#         datasets[time_label][\"SRTM_DEM\"] = dem_ds\n",
        "\n",
        "# # Stack datasets into a single xarray DataArray\n",
        "# stacked_datasets = []\n",
        "# time_labels = sorted(datasets.keys())  # Sort to maintain proper time order\n",
        "\n",
        "# for time_label in time_labels:\n",
        "#     try:\n",
        "#         time_ds = xr.concat([datasets[time_label][var] for var in sorted(all_variables)], dim=\"variable\")\n",
        "#         time_ds = time_ds.assign_coords(time=time_label, variable=list(sorted(all_variables)))\n",
        "#         stacked_datasets.append(time_ds)\n",
        "#     except ValueError as e:\n",
        "#         print(f\"Error concatenating datasets for {time_label}: {e}\")\n",
        "\n",
        "# # Final merging of all time periods\n",
        "# try:\n",
        "#     final_datacube = xr.concat(stacked_datasets, dim=\"time\")\n",
        "#     final_datacube = final_datacube.assign_coords(time=time_labels)  # Ensure proper time labels\n",
        "#     print(\"\\nSuccessfully stacked all datasets into a DataArray!\")\n",
        "# except ValueError as e:\n",
        "#     print(f\"Final concatenation error: {e}\")\n",
        "#     final_datacube = None\n",
        "\n",
        "# # Save the final dataset as NetCDF\n",
        "# if final_datacube is not None and not final_datacube.isnull().all():\n",
        "#     output_path = os.path.join(data_dir, \"datacube.nc\")\n",
        "#     final_datacube.to_netcdf(output_path)\n",
        "#     print(f\"\\nSaved final dataset to: {output_path}\")\n",
        "# else:\n",
        "#     print(\"\\nNo final dataset created due to errors.\")\n"
      ],
      "metadata": {
        "id": "QkCiPwuwEKly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access data for a specific time step/ to check structure (e.g., \"Jan_2025\")\n",
        "time_step = \"Jan_2025\"\n",
        "time_data = final_datacube.sel(time=time_step)\n",
        "\n",
        "print(f\"\\nData for {time_step}:\\n\")\n",
        "print(time_data)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6t9wAAingmbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the available time labels/ steps\n",
        "print(final_datacube.time.values)\n"
      ],
      "metadata": {
        "id": "tNHngDZUFI0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the data for December 2024\n",
        "december_data = final_datacube.sel(time='Dec_2024')\n",
        "\n",
        "# Print available variables for December\n",
        "print(\"Available variables for Dec_2024:\", december_data.variable.values)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "o18Oc3guFfyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set up the figure with 1 row and 2 columns for the plots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# List of variables and corresponding color maps\n",
        "variables = [\"LST\", \"NDVI\"]\n",
        "cmaps = [\"inferno\", \"RdYlGn\"]  # Different color schemes for LST and NDVI\n",
        "\n",
        "# Loop through the variables and plot them\n",
        "for i, (var, cmap) in enumerate(zip(variables, cmaps)):\n",
        "    ax = axes[i]  # Get the current subplot axis\n",
        "    data = final_datacube.sel(time=\"Dec_2024\", variable=var)  # Select data for Dec 2024 and the current variable\n",
        "\n",
        "    # Plot the data with the chosen colormap\n",
        "    im = data.plot(ax=ax, cmap=cmap)\n",
        "\n",
        "    # Update the colorbar to avoid incorrect labels\n",
        "    cbar = im.colorbar\n",
        "    cbar.set_label(var)  # Set the correct label for the colorbar\n",
        "\n",
        "    ax.set_title(f\"{var} - Dec 2024\")  # Set the title for each subplot\n",
        "\n",
        "plt.tight_layout()  # Adjust layout for better spacing\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6aGiqOD5JTSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)\n"
      ],
      "metadata": {
        "id": "f5xSjYPKD5S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.coords[\"variable\"].values) #check for visualization of NDVI variables\n"
      ],
      "metadata": {
        "id": "UDE_-ym2EKlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.coords[\"time\"].values)\n"
      ],
      "metadata": {
        "id": "RkqwQWQgEmGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xarray as xr\n",
        "\n",
        "# Load the NetCDF datacube\n",
        "file_path = \"/content/drive/MyDrive/Amini/datacube.nc\"\n",
        "data = xr.open_dataset(file_path)\n",
        "\n",
        "# Print the dataset structure\n",
        "print(data)\n",
        "\n",
        "# Check the number of time steps\n",
        "if \"time\" in data.dims:\n",
        "    print(f\"Number of time steps: {data.sizes['time']}\")\n",
        "else:\n",
        "    print(\"No 'time' dimension found in the datacube.\")\n"
      ],
      "metadata": {
        "id": "lLn56FzxLDnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xarray as xr\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "dataset = xr.open_dataset(\"/content/drive/MyDrive/Amini/datacube.nc\")\n",
        "\n",
        "# Convert time labels to proper datetime format\n",
        "time_str = dataset[\"time\"].values  # ['Dec_2024', 'Feb_2025', 'Jan_2025']\n",
        "time_dt = pd.to_datetime(time_str, format=\"%b_%Y\")  # Convert to datetime\n",
        "\n",
        "# Assign back to dataset for correct ordering\n",
        "dataset = dataset.assign_coords(time=(\"time\", time_dt))\n",
        "\n",
        "# Sort dataset by time\n",
        "dataset = dataset.sortby(\"time\")\n",
        "\n",
        "# Select NDVI as a DataArray\n",
        "ndvi_data = dataset.sel(variable=\"NDVI\")[\"value\"]  # Extract NDVI values\n",
        "\n",
        "# Extract sorted time values\n",
        "ndvi_times = ndvi_data[\"time\"].values\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, len(ndvi_times), figsize=(15, 5))\n",
        "\n",
        "# If there's only one time step, make axes iterable\n",
        "if len(ndvi_times) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for i, time in enumerate(ndvi_times):\n",
        "    ax = axes[i]\n",
        "    ax.set_title(f\"NDVI on {pd.to_datetime(str(time)).strftime('%b %Y')}\")\n",
        "\n",
        "    # Plot NDVI for each time step\n",
        "    img = ax.imshow(ndvi_data.sel(time=time), cmap=\"RdYlGn\", vmin=-1, vmax=1)\n",
        "    plt.colorbar(img, ax=ax, label=\"NDVI\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "exFL7dFZMdSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xarray as xr\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "dataset = xr.open_dataset(\"/content/drive/MyDrive/Amini/datacube.nc\")\n",
        "\n",
        "# Convert time labels to proper datetime format\n",
        "time_str = dataset[\"time\"].values  # Example: ['Dec_2024', 'Feb_2025', 'Jan_2025']\n",
        "time_dt = pd.to_datetime(time_str, format=\"%b_%Y\")  # Convert to datetime\n",
        "\n",
        "# Assign back to dataset for correct ordering\n",
        "dataset = dataset.assign_coords(time=(\"time\", time_dt))\n",
        "\n",
        "# Sort dataset by time\n",
        "dataset = dataset.sortby(\"time\")\n",
        "\n",
        "# Print dataset structure\n",
        "print(dataset)\n",
        "\n",
        "# Loop through all variables and print corresponding time steps\n",
        "for var in dataset[\"variable\"].values:\n",
        "    data_array = dataset.sel(variable=var)[\"value\"]\n",
        "    print(f\"\\nVariable: {var}\")\n",
        "    for time in dataset[\"time\"].values:\n",
        "        shape = data_array.sel(time=time).shape\n",
        "        print(f\"  Time: {pd.to_datetime(str(time)).strftime('%b %Y')}, Shape: {shape}\")\n"
      ],
      "metadata": {
        "id": "BOI0AA9-PBNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "ds = xr.open_dataset(\"/content/drive/MyDrive/Amini/datacube.nc\")\n"
      ],
      "metadata": {
        "id": "EKyc6CroWgTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xarray as xr\n",
        "\n",
        "# Display dataset summary\n",
        "print(ds)\n",
        "\n",
        "# List all variables in the dataset\n",
        "print(ds.variables)\n",
        "\n",
        "# Show the first few time values to check format\n",
        "print(ds.time.values)\n",
        "\n",
        "# Show sample data for NDVI and LST_Celsius (if they exist)\n",
        "if \"NDVI\" in ds.variables:\n",
        "    print(ds[\"NDVI\"])\n",
        "if \"LST_Celsius\" in ds.variables:\n",
        "    print(ds[\"LST_Celsius\"])\n"
      ],
      "metadata": {
        "id": "gOPJXABxVz4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Ensure dataset is computed\n",
        "ds = ds.compute()\n",
        "\n",
        "# Convert time coordinate to datetime format\n",
        "ds = ds.assign_coords(time=pd.to_datetime(ds.time, format=\"%b_%Y\"))\n",
        "\n",
        "# Select NDVI data\n",
        "ds_ndvi = ds.sel(variable=\"NDVI\").astype(np.float32)\n",
        "\n",
        "# Create figure\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# Initialize NDVI map with imshow instead of xarray plot\n",
        "ndvi_plot = ax.imshow(ds_ndvi.isel(time=0).value, cmap='RdYlGn', vmin=-1, vmax=1)\n",
        "\n",
        "# Add colorbar manually\n",
        "cbar = fig.colorbar(ndvi_plot, ax=ax)\n",
        "cbar.set_label(\"NDVI\")\n",
        "\n",
        "ax.set_title(f\"NDVI Map - {ds_ndvi.time.values[0]}\")\n",
        "\n",
        "# Update function for animation\n",
        "def update(num):\n",
        "    ndvi_plot.set_array(ds_ndvi.isel(time=num).value.values)  # Update NDVI values\n",
        "    ax.set_title(f\"NDVI Map - {pd.to_datetime(str(ds_ndvi.time.values[num])).strftime('%b %Y')}\")\n",
        "    return ndvi_plot,\n",
        "\n",
        "# Create animation\n",
        "ani = animation.FuncAnimation(fig, update, frames=len(ds_ndvi.time), interval=500, blit=False, repeat=True)\n",
        "\n",
        "# Save animation as MP4\n",
        "ani.save(\"ndvi_spatial_animation.mp4\", writer=\"ffmpeg\", fps=1)\n",
        "\n",
        "# Display animation inline (for Jupyter Notebooks)\n",
        "HTML(ani.to_html5_video())\n"
      ],
      "metadata": {
        "id": "xaDRapLzc32H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}